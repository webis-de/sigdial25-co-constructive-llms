{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/bigwork/nhwpficl/.conda/envs/explquality/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import transformers\n",
    "import datasets\n",
    "import glob\n",
    "\n",
    "import torch\n",
    "import json\n",
    "from pathlib import Path\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import (AutoTokenizer, AutoModelForSequenceClassification, default_data_collator,\n",
    "                          DebertaV2ForSequenceClassification, DebertaV2Tokenizer,\n",
    "                          TrainingArguments, Trainer, AutoConfig)\n",
    "from transformers.modeling_outputs import SequenceClassifierOutput\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from tabulate import tabulate\n",
    "\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch \n",
    "import random\n",
    "torch.manual_seed(123)\n",
    "torch.cuda.manual_seed(123)\n",
    "np.random.seed(123)\n",
    "random.seed(123)\n",
    "ros = RandomOverSampler(random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#model_name = \"deberta-v3-large\"\n",
    "model_name = \"longformer-base-4096\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_ds(ds_path, model_name):\n",
    "    df = pd.read_pickle(ds_path)\n",
    "\n",
    "    #Aligning the 5-levels labels to eli5 ones\n",
    "        \n",
    "    #'(D06) To answer - Other' -> '(D06) Answer - Other'\n",
    "    #'(D07) To provide agreement statement' -> '(D07) Agreement'\n",
    "    #'(D08) To provide disagreement statement' -> '(D08) Disagreement'\n",
    "    #'(D10) Other' -> '(D09) Other'\n",
    "    #'(D09) To provide informing statement' -> (D10) To provide informing statement\n",
    "    \n",
    "    \n",
    "    # (E10) Other -> (E09) Other \n",
    "    # (E09) Introducing Extraneous Information -> (E10) Introducing Extraneous Information\n",
    "    \n",
    "    df['exp_act_label'] = df.exp_act_label.apply(lambda x: '(E10) Other' if x == '(E09) Other' else x)\n",
    "    df['exp_act_label'] = df.exp_act_label.apply(lambda x: '(E09) Introducing Extraneous Information' if x == '(E10) Introducing Extraneous Information' else x)\n",
    "\n",
    "    df['dlg_act_label'] = df.dlg_act_label.apply(lambda x: '(D09) Other' if x == '(D10) Other' else x)\n",
    "    df['dlg_act_label'] = df.dlg_act_label.apply(lambda x: '(D10) To provide informing statement' if x == '(D09) To provide informing statement' else x)\n",
    "    \n",
    "    df['dlg_act_label'] = df.dlg_act_label.apply(lambda x: '(D06) Answer - Other' if x == '(D06) To answer - Other' else x)\n",
    "    df['dlg_act_label'] = df.dlg_act_label.apply(lambda x: '(D07) Agreement' if x == '(D07) To provide agreement statement' else x)\n",
    "    df['dlg_act_label'] = df.dlg_act_label.apply(lambda x: '(D08) Disagreement' if x == '(D08) To provide disagreement statement' else x)\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(f\"/bigwork/nhwpficl/hf_models/{model_name}\")\n",
    "\n",
    "    sep_token = tokenizer.sep_token\n",
    "\n",
    "    df['turn_text_with_topic'] = df.apply(lambda row: {\n",
    "                                        'author': row['turn_text']['author'], \n",
    "                                        'text'  : row['topic'].replace('_', ' ') + f' {sep_token} ' +  row['turn_text']['text']\n",
    "                                       } ,axis=1)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fivelvls_annotation_df = load_ds('../../data/five_levels_ds/annotation-results/MACE-measure/final_mace_predictions.pkl', model_name)\n",
    "eli5_annotation_df     = load_ds('../../data/eli5_ds/annotation-results/MACE-measure/final_mace_predictions_training.pkl', model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fivelvls_annotation_df['ds'] = ['5lvls'] * len(fivelvls_annotation_df)\n",
    "eli5_annotation_df['ds'] = ['eli5'] * len(eli5_annotation_df)\n",
    "dlgs_df = pd.concat([fivelvls_annotation_df, eli5_annotation_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ds\n",
       "eli5     2728\n",
       "5lvls    1550\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dlgs_df['ds'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "dlgs_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Experiments:\n",
    "\n",
    "- Train three models, one on eli5, one on 5lvls, and one on both, and evaluate them in three settings, in-domain and out-domain - through a 5-fold cross validation setting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Prepare Folds:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n"
     ]
    }
   ],
   "source": [
    "print(len(dlgs_df[dlgs_df.ds == '5lvls'].topic.value_counts()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "topic\n",
       "dimensions          244\n",
       "gravity             179\n",
       "music_harmony       150\n",
       "sleep_scientist     147\n",
       "lasers              142\n",
       "machine_learning    119\n",
       "origani             112\n",
       "hacking             100\n",
       "nano_technology      92\n",
       "blockchain           91\n",
       "blackhole            70\n",
       "connectome           60\n",
       "virtual_reality      44\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dlgs_df[dlgs_df.ds == '5lvls'].topic.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "154\n"
     ]
    }
   ],
   "source": [
    "print(len(dlgs_df[dlgs_df.ds == 'eli5'].topic.value_counts()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "dlgs_df[dlgs_df.ds == 'eli5'].topic.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#split the two corpora\n",
    "eli5_topics  = dlgs_df[dlgs_df.ds == 'eli5'].topic.unique()\n",
    "flvls_topics = dlgs_df[dlgs_df.ds == '5lvls'].topic.unique()\n",
    "\n",
    "kfold = KFold(n_splits=5)\n",
    "flvls_folds = [(flvls_topics[idx[0]], flvls_topics[idx[1]]) for idx in kfold.split(flvls_topics)]\n",
    "eli5_folds  = [(eli5_topics[idx[0]], eli5_topics[idx[1]]) for idx in kfold.split(eli5_topics)]\n",
    "\n",
    "eli5_training_folds, eli5_test_folds = zip(*eli5_folds)\n",
    "flvls_training_folds, flvls_test_folds = zip(*flvls_folds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "all_folds = {'train':{'5lvls': [x.tolist() for x in flvls_training_folds], 'eli5': [x.tolist() for x in eli5_training_folds]},\n",
    "             'test':{'5lvls': [x.tolist() for x in flvls_test_folds], 'eli5': [x.tolist() for x in eli5_test_folds]}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "json.dump(all_folds, open('../../data/topic_folds.json', 'w'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Training models:\n",
    "\n",
    "Now we train models via command line running turn_label_prediction_experiment_with_bert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code for training the models is in: turn_label_prediction_experiments_with_bert and with_bert_seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Prediction using the trained models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def majority_class(df):\n",
    "    topics = df.topic.unique()\n",
    "    for topic in topics:\n",
    "        training_df = df[df.topic != topic]\n",
    "        #compute the majority class for each label\n",
    "        l = len(df[df.topic == topic])\n",
    "        df.loc[df.topic == topic, 'topic_func_maj_pred'] = [training_df.topic_func_label.mode()] * l\n",
    "        df.loc[df.topic == topic, 'dlg_act_maj_pred']    = [training_df.dlg_act_label.mode()] * l\n",
    "        df.loc[df.topic == topic, 'exp_act_maj_pred']    = [training_df.exp_act_label.mode()] * l\n",
    "    \n",
    "    return df\n",
    "\n",
    "def eval_preds(df, models_names, gt_clms, pred_clms):\n",
    "    results_table = []\n",
    "    for label in zip(gt_clms, pred_clms, models_names):\n",
    "        ground_truths = df[label[0]].tolist()\n",
    "        predictions   = df[label[1]].tolist()\n",
    "        model_name = label[2]\n",
    "        \n",
    "        class_names = df[label[0]].unique()\n",
    "\n",
    "        prc_scores = precision_score(ground_truths, predictions, average=None, labels=class_names)\n",
    "        rec_scores = recall_score(ground_truths, predictions, average=None, labels=class_names)\n",
    "        f1_scores  = f1_score(ground_truths, predictions, average=None, labels=class_names)\n",
    "        \n",
    "        macro_prc_scores = precision_score(predictions, ground_truths, average='macro', labels=class_names)\n",
    "        macro_rec_scores = recall_score(predictions, ground_truths, average='macro', labels=class_names)\n",
    "        macro_f1 = f1_score(predictions, ground_truths, average='macro', labels=class_names)\n",
    "        \n",
    "        scores ={}\n",
    "        for i, c in enumerate(class_names):\n",
    "            scores[c] = {'prec': round(prc_scores[i],2), 'recall': round(rec_scores[i],2), 'f1': round(f1_scores[i],2)}\n",
    "        \n",
    "        scores['Macro AVG.'] = {'prec': round(macro_prc_scores,2), 'recall': round(macro_rec_scores,2), 'f1': round(macro_f1,2)}\n",
    "        \n",
    "        results_table.append([model_name, label[0], scores])\n",
    "    \n",
    "    return results_table\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = predictions.argmax(-1)\n",
    "    f1score = f1_score(predictions, labels, average='macro')\n",
    "    return {'f1-score': f1score}\n",
    "\n",
    "def evaluate_fold(fold_path, df, fold_idx, input_clm, tokenizer, label_clm='exp_act_label', ds_names=[]):\n",
    "    print(fold_path)\n",
    "    config = AutoConfig.from_pretrained(f\"/bigwork/nhwpficl/hf_models/{model_name}\")\n",
    "    if \"deberta-v3\" in model_name:\n",
    "        model = DebertaV2ForSequenceClassification.from_pretrained(fold_path + 'best_model').to(device)\n",
    "        max_length = config.max_position_embeddings * 2\n",
    "    else:\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(fold_path + 'best_model').to(device)\n",
    "        max_length = config.max_position_embeddings - 2\n",
    "    \n",
    "    model.eval()\n",
    "    fold_topics = []\n",
    "    if 'eli5' in ds_names:\n",
    "        fold_topics += folds_dict['test']['eli5'][fold_idx]\n",
    "    if '5lvls' in ds_names:\n",
    "        fold_topics += folds_dict['test']['5lvls'][fold_idx]\n",
    "    \n",
    "    test_df  = df[df.topic.isin(fold_topics)]\n",
    "    test_df['labels'] = test_df[label_clm].apply(lambda x: int(x[2:4])-1) \n",
    "    \n",
    "    eval_dataset = Dataset.from_pandas(test_df)\n",
    "    eval_dataset = eval_dataset.map(lambda examples: tokenizer([x['text'] for x in examples[input_clm]], padding='max_length', max_length=max_length), batched=True)\n",
    "    eval_dataset = eval_dataset.remove_columns(test_df.columns.tolist() + ['__index_level_0__'])\n",
    "    \n",
    "    \n",
    "    eval_dataloader = DataLoader(eval_dataset, collate_fn=default_data_collator)\n",
    "    all_predictions = []\n",
    "    for step, batch in enumerate(eval_dataloader):\n",
    "        batch = {x[0]: x[1].cuda() for x in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        predictions = outputs.logits.argmax(dim=-1)\n",
    "        all_predictions+=[x.item() for x in predictions]\n",
    "\n",
    "    #print(all_predictions)\n",
    "    #print(test_df['labels'].tolist())\n",
    "    \n",
    "    return f1_score(all_predictions, test_df['labels'].tolist(), average='macro')\n",
    "\n",
    "def evaluate_model(models_path, label_clm, label_model, model_name='bert-base-uncased'):\n",
    "    f1_scores = {}\n",
    "\n",
    "    if \"deberta-v3\" in model_name:\n",
    "        tokenizer = DebertaV2Tokenizer.from_pretrained(f\"/bigwork/nhwpficl/hf_models/{model_name}\")\n",
    "    else:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(f\"/bigwork/nhwpficl/hf_models/{model_name}\")\n",
    "    \n",
    "    for model_type in ['all_models']:\n",
    "        model_scores = []\n",
    "        for ds_names in [['eli5'], ['5lvls'], ['5lvls', 'eli5']]:\n",
    "            s = np.mean([evaluate_fold('{}/{}/{}/model/fold-{}/'.format(models_path, label_model, model_type, fold), dlgs_df.copy(), \n",
    "                                       fold, 'turn_text_with_topic', tokenizer, label_clm, ds_names=ds_names) \n",
    "                         for fold in range(5)])\n",
    "            model_scores.append(round(s, 2))\n",
    "        f1_scores[model_name] = model_scores\n",
    "    \n",
    "    return f1_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "folds_dict = json.load(open('../../data/topic_folds.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "dlg_act_f1_scores = evaluate_model(f'../../data/turn-label-models/{model_name}/', 'dlg_act_label', 'dlg_act_label_prediction', model_name=model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "exp_act_f1_scores = evaluate_model(f'../../data/turn-label-models/{model_name}/', 'exp_act_label', 'exp_act_label_prediction', model_name=model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "topic_func_f1_scores = evaluate_model(f'../../data/turn-label-models/{model_name}/', 'topic_func_label', 'topic_func_label_prediction', model_name=model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Approach                ELI5    5lvls    ALL    ELI5    5lvls    ALL    ELI5    5lvls    ALL\n",
      "--------------------  ------  -------  -----  ------  -------  -----  ------  -------  -----\n",
      "longformer-base-4096    0.37     0.38   0.41    0.38     0.47   0.48    0.38     0.56    0.5\n"
     ]
    }
   ],
   "source": [
    "print(tabulate([[x[0][0]] + x[0][1] + x[1][1] + x[2][1] for x in zip(exp_act_f1_scores.items(), dlg_act_f1_scores.items(), topic_func_f1_scores.items())], \n",
    "               headers=['Approach', 'ELI5', '5lvls', 'ALL', 'ELI5', '5lvls', 'ALL', 'ELI5', '5lvls', 'ALL']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting on the test set:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Best Performing Models:\n",
    "    - Explanation Moves: ELI-5 trained BERT-Seq\n",
    "    - Dialogue Acts: Both trained RoBERTa\n",
    "    - Topic Func: ELI-5 trained RoBERTa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_ds(ds_path, model_name):\n",
    "    if \"pkl\" in ds_path:\n",
    "        df = pd.read_pickle(ds_path)\n",
    "    else:\n",
    "        with open(f\"{ds_path}/chat_per_user.json\", \"r\") as file:\n",
    "            chat_per_user = json.load(file)\n",
    "        with open(f\"{ds_path}/setup_per_user.json\", \"r\") as file:\n",
    "            setup_per_user = json.load(file)\n",
    "        \n",
    "        turns = []\n",
    "        for user_id in chat_per_user:\n",
    "            for turn in chat_per_user[user_id]:\n",
    "                turn[\"task_id\"] = user_id\n",
    "                turn[\"topic\"] = setup_per_user[user_id][\"explanandum\"]\n",
    "                turn[\"system_prompt\"] = setup_per_user[user_id][\"setting\"]\n",
    "                turns.append(turn)\n",
    "        df = pd.DataFrame.from_dict(turns)\n",
    "\n",
    "    #Aligning the 5-levels labels to eli5 ones\n",
    "        \n",
    "    #'(D06) To answer - Other' -> '(D06) Answer - Other'\n",
    "    #'(D07) To provide agreement statement' -> '(D07) Agreement'\n",
    "    #'(D08) To provide disagreement statement' -> '(D08) Disagreement'\n",
    "    #'(D10) Other' -> '(D09) Other'\n",
    "    #'(D09) To provide informing statement' -> (D10) To provide informing statement\n",
    "    \n",
    "    \n",
    "    # (E10) Other -> (E09) Other \n",
    "    # (E09) Introducing Extraneous Information -> (E10) Introducing Extraneous Information\n",
    "    \n",
    "    if 'exp_act_label' in df:\n",
    "        df['exp_act_label'] = df.exp_act_label.apply(lambda x: '(E10) Other' if x == '(E09) Other' else x)\n",
    "        df['exp_act_label'] = df.exp_act_label.apply(lambda x: '(E09) Introducing Extraneous Information' if x == '(E10) Introducing Extraneous Information' else x)\n",
    "\n",
    "        df['dlg_act_label'] = df.dlg_act_label.apply(lambda x: '(D09) Other' if x == '(D10) Other' else x)\n",
    "        df['dlg_act_label'] = df.dlg_act_label.apply(lambda x: '(D10) To provide informing statement' if x == '(D09) To provide informing statement' else x)\n",
    "\n",
    "        df['dlg_act_label'] = df.dlg_act_label.apply(lambda x: '(D06) Answer - Other' if x == '(D06) To answer - Other' else x)\n",
    "        df['dlg_act_label'] = df.dlg_act_label.apply(lambda x: '(D07) Agreement' if x == '(D07) To provide agreement statement' else x)\n",
    "        df['dlg_act_label'] = df.dlg_act_label.apply(lambda x: '(D08) Disagreement' if x == '(D08) To provide disagreement statement' else x)\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(f\"/bigwork/nhwpficl/hf_models/{model_name}\")\n",
    "\n",
    "    sep_token = tokenizer.sep_token\n",
    "\n",
    "    df['turn_text_with_topic'] = df.apply(lambda row: {\n",
    "                                        'author': row['turn_text']['author'], \n",
    "                                        'text'  : row['topic'].replace('_', ' ') + f' {sep_token} ' +  row['turn_text']['text']\n",
    "                                       } ,axis=1)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Loading and preparing data\n",
    "model_name = \"longformer-base-4096\"\n",
    "fivelvls_annotation_df = load_ds('../../data/five_levels_ds/annotation-results/MACE-measure/final_mace_predictions.pkl', model_name)\n",
    "eli5_annotation_df     = load_ds('../../data/eli5_ds/annotation-results/MACE-measure/final_mace_predictions.pkl', model_name)\n",
    "\n",
    "fivelvls_annotation_df['ds'] = ['5lvls'] * len(fivelvls_annotation_df)\n",
    "eli5_annotation_df['ds'] = ['eli5'] * len(eli5_annotation_df)\n",
    "dlgs_df = pd.concat([fivelvls_annotation_df, eli5_annotation_df])\n",
    "\n",
    "#split into train test split\n",
    "#train_test_topics = {\"test\": {}}\n",
    "#for dataset in ['eli5', '5lvls']:\n",
    "#    topics = dlgs_df[dlgs_df.ds == dataset].topic.unique()\n",
    "#    _, valid_topics = train_test_split(topics, shuffle=False, test_size=0.2, random_state=0)\n",
    "#    train_test_topics[\"test\"][dataset] = list(valid_topics)\n",
    "#test_df  = dlgs_df[dlgs_df.topic.isin(train_test_topics['test']['5lvls'] + train_test_topics['test']['eli5'])]\n",
    "#test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_df = load_ds('../../../evaluation/user_study_data', model_name)\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def ensemble_generate(models_path, test_df, input_clm, label_clm, label_model, model_name):\n",
    "    fold_path = '{}/{}/{}/model/'.format(models_path, label_model, 'all_models')\n",
    "    print(fold_path)\n",
    "    label_dictionary = {int(l[2:4])-1 : l for l in  dlgs_df[label_clm].unique()}\n",
    "    print(label_dictionary)\n",
    "    \n",
    "    config = AutoConfig.from_pretrained(f\"/bigwork/nhwpficl/hf_models/{model_name}\")\n",
    "    if \"deberta-v3\" in model_name:\n",
    "        model = DebertaV2ForSequenceClassification.from_pretrained(fold_path + 'best_model').to(device)\n",
    "        max_length = config.max_position_embeddings * 2\n",
    "        tokenizer = DebertaV2Tokenizer.from_pretrained(f\"/bigwork/nhwpficl/hf_models/{model_name}\")\n",
    "    else:\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(fold_path + 'best_model').to(device)\n",
    "        max_length = config.max_position_embeddings - 2\n",
    "        tokenizer = AutoTokenizer.from_pretrained(f\"/bigwork/nhwpficl/hf_models/{model_name}\")\n",
    "        \n",
    "    eval_dataset = Dataset.from_pandas(test_df)\n",
    "    eval_dataset = eval_dataset.map(lambda examples: tokenizer([str(x['text']) for x in examples[input_clm]], padding='max_length', max_length=max_length), batched=True)\n",
    "    eval_dataset = eval_dataset.remove_columns(test_df.columns.tolist())\n",
    "    \n",
    "    \n",
    "    eval_dataloader = DataLoader(eval_dataset, collate_fn=default_data_collator, batch_size=4)\n",
    "    all_predictions = []\n",
    "    for step, batch in enumerate(eval_dataloader):\n",
    "        batch = {x[0]: x[1].cuda() for x in batch.items()}\n",
    "        \n",
    "        model_outputs = model(**batch)\n",
    "        model_predictions = model_outputs.logits.argmax(dim=-1)\n",
    "        model_predictions = [x.item() for x in model_predictions]\n",
    "        all_predictions += model_predictions\n",
    "        #print(batch['input_ids'].shape)\n",
    "        #print(models_outputs)\n",
    "        #print(models_predictions)\n",
    "\n",
    "    test_df[label_clm + '_predictions'] = [label_dictionary[x] for x in all_predictions]\n",
    "    return test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_df = ensemble_generate(f'../../data/final-turn-label-models/{model_name}/', test_df, 'turn_text_with_topic', 'dlg_act_label', 'dlg_act_label_prediction', model_name=model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_df = ensemble_generate(f'../../data/final-turn-label-models/{model_name}/', test_df, 'turn_text_with_topic', 'exp_act_label', 'exp_act_label_prediction', model_name=model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_df = ensemble_generate(f'../../data/final-turn-label-models/{model_name}/', test_df, 'turn_text_with_topic', 'topic_func_label', 'topic_func_label_prediction', model_name=model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_df.to_pickle(f'../../data/final_mace_predictions_{model_name}.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "explquality_python312",
   "language": "python",
   "name": "explquality_python312"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
